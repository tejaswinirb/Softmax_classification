{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "source": [
        "## Load libraries\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q1e2N5S8MlCU",
        "outputId": "7b7a3f24-7dd5-4dcf-8dfa-3d4241856ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount the Google Drive folder, if needed, for accessing data\n",
        "if('google.colab' in sys.modules):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    # Change path below starting from /content/drive/MyDrive/Colab Notebooks/\n",
        "    # depending on how data is organized inside your Colab Notebooks folder in\n",
        "    # Google Drive\n",
        "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/OddSem2023MAHE'\n",
        "    DATA_DIR = DIR+'/Data/'\n",
        "else:\n",
        "    DATA_DIR = 'Data/'"
      ],
      "metadata": {
        "id": "69kMVA0KlRwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2618ad-0d0a-4b6e-aa3d-fac4288ca871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate artificial data with 5 samples, 4 features per sample\n",
        "# and 3 output classes\n",
        "num_samples = 5 # number of samples\n",
        "num_features = 4 # number of features (a.k.a. dimensionality)\n",
        "num_labels = 3 # number of output labels\n",
        "# Data matrix (each column = single sample)\n",
        "X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n",
        "# Class labels\n",
        "y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n",
        "print(X)\n",
        "print('------')\n",
        "print(y)\n",
        "print('------')\n",
        "# One-hot encode class labels\n",
        "y = tf.keras.utils.to_categorical(y)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4EjOi-OM4Gp",
        "outputId": "a2f44aca-1a46-43bf-9092-993898ef9b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5 6 4 6 7]\n",
            " [5 8 7 9 8]\n",
            " [9 4 3 3 6]\n",
            " [7 5 8 9 8]]\n",
            "------\n",
            "[2 0 2 0 2]\n",
            "------\n",
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "IrXipxwrJ0_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ],
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "The softmax classifier steps for a generic sample $\\mathbf{x}$ with (one-hot encoded) true label $\\mathbf{y}$ (3 possible categories) using a randomly initialized weights matrix (with bias abosrbed as its last last column):\n",
        "\n",
        "1. Calculate raw scores vector $$\\mathbf{z} = \\mathbf{Wx}.$$\n",
        "2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_1\\\\a_2\\\\a_3\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_1\\\\z_2\\\\z_3\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}\\\\\\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}\\\\\\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}\\end{bmatrix}$$\n",
        "3. Predicted probability vector that the sample belongs to each one of the putput categories is $\\hat{\\mathbf{y}} = \\mathbf{a}.$\n",
        "4. Softmax loss for this sample is\n",
        "$$\\begin{align*}L &=  -\\log(a_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right)\\\\\\Rightarrow L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)&=\\sum_{k=1}^3-y_k\\log\\left(\\hat{y}_k\\right)\\end{align*},$$\n",
        "which is also referred to as the categorical crossentropy loss.\n",
        "5. Calculate the gradient of loss w.r.t. weights:\n",
        "$$\\begin{align*} L\\\\\\downarrow\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\\\downarrow\\\\\\mathbf{z}\\\\\\downarrow\\\\\\mathbf{W}\\end{align*}$$\n",
        "$$\\begin{align*}\\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\mathbf{a})\\times\\nabla_\\mathbf{a}(L).\\end{align*}$$\n",
        "\n",
        "$\\nabla_\\mathbf{z}(\\mathbf{a}) = \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_1\\\\a_2\\\\a_3\\end{bmatrix}\\right) = \\begin{bmatrix}\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)&\\nabla_\\mathbf{z}(a_3)\\end{bmatrix}.$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vdLfiQSlOSUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(tf.nn.softmax([-1., 0., 1.]))"
      ],
      "metadata": {
        "id": "XOUHVIQPLHGG",
        "outputId": "1a9014a8-d388-4cba-aaa4-c27e01ca1574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09003057, 0.24472848, 0.66524094], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: softmax activation class\n",
        "\n",
        "class Softmax(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = np.array(tf.nn.softmax(self.input))\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    return(np.dot((np.identity(np.size(self.output))-self.output.T) * self.output, output_gradient))\n"
      ],
      "metadata": {
        "id": "hGf4rsfELgVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cce(y, yhat):\n",
        "  return(-np.sum(y*np.log(yhat)))\n",
        "def cce_gradient(y, yhat):\n",
        "  return(-y/yhat)"
      ],
      "metadata": {
        "id": "6q9AcNUCRpKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array([0, 1, 0])\n",
        "yhat =np.array([0.75, 0.1, 0.15])\n",
        "print(cce(y, yhat))"
      ],
      "metadata": {
        "id": "M_FOvSqAR1DM",
        "outputId": "dd15d4a7-3db3-4743-af22-6ada5dc43b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3025850929940455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1: Add the bias feature to all the samples\n",
        "X = np.vstack([X, np.ones(X.shape[1])])\n",
        "\n",
        "# Step-2: Initialize the entries of the weights matrix randomly\n",
        "np.random.seed(42)\n",
        "W = np.random.rand(num_labels, num_features + 1)  # +1 for the bias feature\n",
        "\n",
        "# Step-3: Create softmax layer object\n",
        "softmax = Softmax()\n",
        "\n",
        "# Step-4: Run over each sample\n",
        "for i in range(X.shape[1]):\n",
        "    # Step-5: Forward step\n",
        "    # (a) Calculate the raw scores vector for a generic sample\n",
        "    z = np.dot(W, X[:, i])\n",
        "\n",
        "    # (b) Softmax activation\n",
        "    softmax.forward(z)\n",
        "\n",
        "    # (c) Calculate categorical crossentropy (CCE) loss for the sample\n",
        "    loss = cce(y[i, :], softmax.output)\n",
        "\n",
        "    # Step-6: Backward step\n",
        "    # (a) Compute the gradient of the loss for the sample w.r.t. weights\n",
        "    gradient = softmax.backward(cce_gradient(y[i, :], softmax.output), learning_rate=0.01)\n",
        "\n",
        "    # (d) Print CCE loss\n",
        "    print(f\"Sample {i + 1} - CCE Loss: {loss}\")\n",
        "\n",
        "    # (e) Print gradient\n",
        "    print(f\"Sample {i + 1} - Gradient: {gradient}\")\n",
        "\n",
        "    # Step-7: Update weights using gradient descent\n",
        "    W -= 0.01 * gradient\n",
        "\n",
        "# Print the final weights\n",
        "print(\"\\nFinal Weights:\")\n",
        "print(W)\n"
      ],
      "metadata": {
        "id": "Vhy3zsq_p5_4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}